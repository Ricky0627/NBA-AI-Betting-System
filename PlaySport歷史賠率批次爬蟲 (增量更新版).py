import proxy_patch
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import datetime
import random
import os
import re

# --- 1. éšŠåå°ç…§è¡¨ ---
TEAM_MAP = {
    'è€é·¹': 'ATL', 'å¡çˆ¾æå…‹': 'BOS', 'å¡çˆ¾æ': 'BOS',
    'ç±ƒç¶²': 'BRK', 'é»ƒèœ‚': 'CHO',
    'å…¬ç‰›': 'CHI', 'é¨å£«': 'CLE', 'ç¨è¡Œä¿ ': 'DAL', 'é‡‘å¡Š': 'DEN',
    'æ´»å¡': 'DET', 'å‹‡å£«': 'GSW', 'ç«ç®­': 'HOU', 'æºœé¦¬': 'IND',
    'å¿«è‰‡': 'LAC', 'æ¹–äºº': 'LAL', 'ç°ç†Š': 'MEM', 'ç†±ç«': 'MIA',
    'å…¬é¹¿': 'MIL', 'ç°ç‹¼': 'MIN', 'éµœé¶˜': 'NOP', 'å°¼å…‹': 'NYK',
    'é›·éœ†': 'OKC', 'é­”è¡“': 'ORL', '76äºº': 'PHI', 'ä¸ƒå…­äºº': 'PHI',
    'å¤ªé™½': 'PHO', 'æ‹“è’è€…': 'POR', 'æ‹“è’': 'POR',
    'åœ‹ç‹': 'SAC', 'é¦¬åˆº': 'SAS', 'æš´é¾': 'TOR',
    'çˆµå£«': 'UTA', 'å·«å¸«': 'WAS'
}

def parse_odds_from_td(td_tag):
    """å¾è³ ç‡æ ¼å­ä¸­æå–æ•¸å­—"""
    if not td_tag:
        return None
    try:
        # ç§»é™¤ HTML æ¨™ç±¤ï¼Œåªç•™æ–‡å­—
        text = td_tag.get_text().strip()
        # å°‹æ‰¾æµ®é»æ•¸ (å¦‚ 1.75, 2.25)
        # æ’é™¤æ—¥æœŸæ ¼å¼æˆ–ç™¾åˆ†æ¯”
        matches = re.findall(r"(\d+\.\d+)", text)
        for m in matches:
            val = float(m)
            # é‹å½©ä¸è®“åˆ†è³ ç‡é€šå¸¸åœ¨ 1.01 åˆ° 15.0 ä¹‹é–“
            if 1.01 <= val <= 15.0:
                return val
    except:
        pass
    return None

def get_playsport_odds_robust(target_date_str):
    """
    æŠ“å–å–®æ—¥è³ ç‡ (é‡å° gamesData/result æ­·å²è³½æœé é¢å„ªåŒ–ç‰ˆ)
    target_date_str: YYYYMMDD (é€™æ˜¯å°ç£æ™‚é–“)
    """
    url = f"https://www.playsport.cc/gamesData/result?allianceid=3&gametime={target_date_str}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8' # å¼·åˆ¶ UTF-8
        
        if resp.status_code != 200:
            print(f"   âš ï¸ è«‹æ±‚å¤±æ•— ({resp.status_code}) - URL: {url}")
            return []
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # --- è¨ˆç®—ç¾åœ‹æ™‚é–“ (ç”¨æ–¼å­˜æª”) ---
        tw_date = datetime.datetime.strptime(target_date_str, "%Y%m%d").date()
        us_date = tw_date - datetime.timedelta(days=1)
        date_for_save = us_date.strftime("%Y-%m-%d")
        # ---------------------------

        games_data = []
        
        # å°‹æ‰¾æ‰€æœ‰æ¯”è³½çš„ç¬¬ä¸€è¡Œ (åŒ…å« td-teaminfo çš„é‚£ä¸€è¡Œ)
        # é€™äº› tr é€šå¸¸å¸¶æœ‰ gameid å±¬æ€§
        rows = soup.find_all('tr', attrs={'gameid': True})
        
        # ä½¿ç”¨ set é¿å…é‡è¤‡è™•ç† (å› ç‚ºæœ‰äº›çµæ§‹å¯èƒ½æœ‰åµŒå¥—)
        processed_gameids = set()

        for row in rows:
            game_id = row['gameid']
            if game_id in processed_gameids:
                continue
            
            # æª¢æŸ¥é€™ä¸€è¡Œæ˜¯å¦åŒ…å«çƒéšŠè³‡è¨Š (td-teaminfo)
            # åªæœ‰åŒ…å«çƒéšŠè³‡è¨Šçš„è¡Œï¼Œæ‰æ˜¯æˆ‘å€‘è™•ç†çš„èµ·é» (å®¢éšŠè¡Œ)
            team_info_td = row.find('td', class_='td-teaminfo')
            if not team_info_td:
                continue # å¦‚æœé€™è¡Œæ²’æœ‰çƒéšŠè³‡è¨Šï¼Œè·³é (é€™å¯èƒ½æ˜¯ä¸»éšŠè¡Œï¼Œç¨å¾Œæœƒè¢«è‡ªå‹•æŠ“å–)
            
            # --- 1. è§£æéšŠå ---
            # td-teaminfo è£¡é¢åŒ…å«å…©å€‹éšŠä¼çš„é€£çµæˆ–æ–‡å­—
            # é †åºé€šå¸¸æ˜¯ï¼šä¸Šç‚ºå®¢éšŠï¼Œä¸‹ç‚ºä¸»éšŠ
            # æˆ‘å€‘ç›´æ¥æŠ“å–è©²æ ¼å­å…§çš„æ‰€æœ‰æ–‡å­—ï¼Œä¸¦ä¾åºæ¯”å°
            all_text = team_info_td.get_text()
            
            found_teams = []
            # é€™è£¡æˆ‘å€‘éœ€è¦ä¿ç•™é †åºï¼Œæ‰€ä»¥ä¸èƒ½ç”¨å­—å…¸è¿­ä»£ï¼Œæ”¹ç”¨æƒææ–‡å­—çš„æ–¹å¼
            # ä½†æœ€ç°¡å–®çš„æ–¹æ³•æ˜¯ï¼šæ‰¾å‡ºæ‰€æœ‰ç¬¦åˆæˆ‘å€‘ TEAM_MAP çš„é—œéµå­—
            # ç‚ºäº†é¿å…èª¤åˆ¤ (ä¾‹å¦‚ "å…¬ç‰›" å’Œ "å°ç‰›"?)ï¼Œæˆ‘å€‘ç›´æ¥å°‹æ‰¾ map ä¸­çš„ key
            
            # æ›´å¥½çš„æ–¹æ³•ï¼šæ‰¾è£¡é¢çš„ <a> æ¨™ç±¤ï¼Œé€šå¸¸éšŠåéƒ½åœ¨ <a> è£¡é¢
            links = team_info_td.find_all('a')
            for link in links:
                t_text = link.get_text().strip()
                for name, code in TEAM_MAP.items():
                    if name in t_text:
                        found_teams.append(code)
                        break # æ‰¾åˆ°å°æ‡‰ä»£ç¢¼å°±æ›ä¸‹ä¸€å€‹ link
            
            # å¦‚æœç”¨ <a> æ‰¾ä¸åˆ° (æœ‰æ™‚å¯èƒ½æ²’é€£çµ)ï¼Œå†è©¦è©¦ç´”æ–‡å­—æš´åŠ›æœå°‹
            if len(found_teams) < 2:
                found_teams = []
                for name, code in TEAM_MAP.items():
                    if name in all_text:
                        # é€™è£¡æœ‰å€‹å°å•é¡Œï¼šå¦‚æœæ–‡å­—æ˜¯ "æ´›æ‰ç£¯æ¹–äºº"ï¼Œ"æ¹–äºº" æœƒè¢«å°åˆ°ã€‚
                        # æˆ‘å€‘å‡è¨­ map è£¡çš„ key æ˜¯è¶³å¤ ç¨ç‰¹çš„
                        # ç‚ºäº†ç¢ºä¿é †åºï¼Œé€™æœ‰é»é›£ï¼Œæš«æ™‚å‡è¨­ <a> æ¨™ç±¤è§£ææˆåŠŸç‡è¼ƒé«˜
                        # è‹¥çœŸçš„å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼š
                        pass

            # è‹¥é‚„æ˜¯æŠ“ä¸åˆ°å…©éšŠï¼Œè·³é
            if len(found_teams) < 2:
                # Debug: print(f"   è·³é: æŠ“ä¸åˆ°å…©å€‹éšŠå - {all_text.strip()[:20]}...")
                continue
            
            # æŒ‰ç…§ PlaySport è³½æœé é¢æ…£ä¾‹ï¼šç¬¬ä¸€å€‹æ˜¯å®¢éšŠï¼Œç¬¬äºŒå€‹æ˜¯ä¸»éšŠ
            away_code = found_teams[0]
            home_code = found_teams[1]
            
            if away_code == home_code:
                continue

            # --- 2. è§£æè³ ç‡ ---
            # å®¢éšŠè³ ç‡ï¼šåœ¨ç•¶å‰è¡Œ (row) çš„ td-bank-bet03
            # ä¸»éšŠè³ ç‡ï¼šåœ¨ä¸‹ä¸€è¡Œ (next_sibling) çš„ td-bank-bet03
            
            # å®¢éšŠè³ ç‡
            odds_away = parse_odds_from_td(row.find('td', class_='td-bank-bet03'))
            
            # ä¸»éšŠè³ ç‡ (å°‹æ‰¾ä¸‹ä¸€å€‹ tr)
            next_row = row.find_next_sibling('tr')
            odds_home = None
            if next_row and next_row.get('gameid') == game_id:
                odds_home = parse_odds_from_td(next_row.find('td', class_='td-bank-bet03'))
            
            # è‹¥ä¸‹ä¸€è¡Œæ‰¾ä¸åˆ°ï¼Œæœ‰æ™‚å€™å¯èƒ½æ˜¯çµæ§‹å•é¡Œï¼Œå†è©¦è©¦ next_sibling çš„ next_sibling
            # ä½†é€šå¸¸ PlaySport çµæ§‹å¾ˆå›ºå®š
            
            if odds_away and odds_home:
                games_data.append({
                    'Date': date_for_save,
                    'Away_Abbr': away_code,
                    'Home_Abbr': home_code,
                    'Odds_Away': odds_away,
                    'Odds_Home': odds_home
                })
                # æ¨™è¨˜æ­¤ game_id å·²è™•ç†
                processed_gameids.add(game_id)
        
        return games_data

    except Exception as e:
        print(f"   âš ï¸ è§£æéŒ¯èª¤: {e}")
        return []

def scrape_playsport_history(start_date, end_date):
    """æ‰¹æ¬¡æŠ“å–ç¯„åœå…§çš„è³ ç‡"""
    all_data = []
    current = start_date
    
    while current <= end_date:
        date_str = current.strftime("%Y%m%d")
        
        # é¡¯ç¤ºæ­£åœ¨æŠ“å–ï¼Œä¸¦æç¤ºå°æ‡‰çš„ç¾åœ‹æ—¥æœŸ
        us_date_display = current - datetime.timedelta(days=1)
        print(f"   æ­£åœ¨æŠ“å–: {date_str} (å°æ‡‰ç¾åœ‹æ™‚é–“ {us_date_display}) ...")
        
        daily_data = get_playsport_odds_robust(date_str)
        if daily_data:
            print(f"     -> æˆåŠŸæŠ“å– {len(daily_data)} å ´")
            all_data.extend(daily_data)
        else:
            print("     -> ç„¡è³‡æ–™æˆ–è§£æå¤±æ•—")
            
        current += datetime.timedelta(days=1)
        # éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«é– IP
        time.sleep(random.uniform(1.0, 2.0)) 
        
    return all_data

# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    filename = "odds_2026_full_season.csv"
    
    # è³½å­£é–‹å§‹æ—¥æœŸ (2025-10-22 å°ç£æ™‚é–“)
    season_start = datetime.date(2025, 10, 22) 
    end_date = datetime.date.today() + datetime.timedelta(days=1) 

    existing_df = pd.DataFrame()
    start_date = season_start

    if os.path.exists(filename):
        try:
            existing_df = pd.read_csv(filename)
            if not existing_df.empty:
                last_us_date_str = existing_df['Date'].max()
                last_us_date = datetime.datetime.strptime(last_us_date_str, "%Y-%m-%d").date()
                
                # å¢é‡æ›´æ–°é‚è¼¯
                start_date = last_us_date + datetime.timedelta(days=2) 
                
                print(f"ğŸ“‚ ç™¼ç¾èˆŠæª”æ¡ˆï¼Œæœ€å¾Œè¨˜éŒ„æ—¥æœŸ (US): {last_us_date_str}")
                print(f"   -> ä¸Šæ¬¡çˆ¬å–çš„å°ç£æ—¥æœŸæ‡‰ç‚º: {last_us_date + datetime.timedelta(days=1)}")
                print(f"   -> æœ¬æ¬¡å°‡å¾å°ç£æ™‚é–“ {start_date} é–‹å§‹æ›´æ–°")
            else:
                print("   âš ï¸ æª”æ¡ˆä¼¼ä¹ç‚ºç©ºï¼Œé‡æ–°æŠ“å–ã€‚")
        except:
            print("   âš ï¸ è®€å–å¤±æ•—ï¼Œé‡æ–°æŠ“å–ã€‚")
            
    if start_date > end_date:
        print(f"âœ… æ•¸æ“šå·²æ˜¯æœ€æ–° (å·²æ¶µè“‹è‡³å°ç£æ™‚é–“ {end_date})ï¼Œç„¡éœ€æ›´æ–°ã€‚")
    else:
        print(f"\nğŸš€ é–‹å§‹æ›´æ–° 2026 è³½å­£æ•¸æ“šï¼Œç¯„åœ: {start_date} ~ {end_date}")
        print("=" * 60)
        
        new_data = scrape_playsport_history(start_date, end_date)
        
        if new_data:
            new_df = pd.DataFrame(new_data)
            final_df = pd.concat([existing_df, new_df]).drop_duplicates(subset=['Date', 'Home_Abbr', 'Away_Abbr'], keep='last')
            final_df = final_df.sort_values('Date')
            
            final_df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ‰ æ›´æ–°å®Œæˆï¼ç¸½ç­†æ•¸: {len(final_df)}")
            print(f"æª”æ¡ˆå·²å„²å­˜è‡³: {filename}")
        else:
            print("âš ï¸ æœ¬æ¬¡ç„¡æ–°æ•¸æ“šæ›´æ–°ã€‚")