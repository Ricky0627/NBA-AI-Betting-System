import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import datetime
import random
import os  # æ–°å¢ï¼šç”¨æ–¼æª¢æŸ¥æª”æ¡ˆ

# --- 1. éšŠåå°ç…§è¡¨ ---
TEAM_MAP = {
    'è€é·¹': 'ATL', 'å¡çˆ¾æå…‹': 'BOS', 'å¡çˆ¾æ': 'BOS',
    'ç±ƒç¶²': 'BRK', 'é»ƒèœ‚': 'CHO',
    'å…¬ç‰›': 'CHI', 'é¨å£«': 'CLE', 'ç¨è¡Œä¿ ': 'DAL', 'é‡‘å¡Š': 'DEN',
    'æ´»å¡': 'DET', 'å‹‡å£«': 'GSW', 'ç«ç®­': 'HOU', 'æºœé¦¬': 'IND',
    'å¿«è‰‡': 'LAC', 'æ¹–äºº': 'LAL', 'ç°ç†Š': 'MEM', 'ç†±ç«': 'MIA',
    'å…¬é¹¿': 'MIL', 'ç°ç‹¼': 'MIN', 'éµœé¶˜': 'NOP', 'å°¼å…‹': 'NYK',
    'é›·éœ†': 'OKC', 'é­”è¡“': 'ORL', '76äºº': 'PHI', 'ä¸ƒå…­äºº': 'PHI',
    'å¤ªé™½': 'PHO', 'æ‹“è’è€…': 'POR', 'æ‹“è’': 'POR',
    'åœ‹ç‹': 'SAC', 'é¦¬åˆº': 'SAS', 'æš´é¾': 'TOR',
    'çˆµå£«': 'UTA', 'å·«å¸«': 'WAS'
}

def get_playsport_odds_robust(target_date_str):
    """
    æŠ“å–å–®æ—¥è³ ç‡ (é‚è¼¯ä¸è®Š)
    target_date_str: YYYYMMDD
    """
    url = f"https://www.playsport.cc/gamesData/result?allianceid=3&gametime={target_date_str}"
    # print(f"  æ­£åœ¨æŠ“å–: {target_date_str} ...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'lxml')
        
        game_rows = soup.find_all('tr', attrs={'gameid': True})
        if not game_rows:
            main_table = soup.find('table', class_='predictgame-table')
            if main_table:
                game_rows = main_table.find_all('tr', attrs={'gameid': True})
        
        if not game_rows:
            return []
            
        games_dict = {}
        for row in game_rows:
            gid = row['gameid']
            if gid not in games_dict: games_dict[gid] = []
            games_dict[gid].append(row)
            
        daily_data = []
        
        for gid, rows in games_dict.items():
            if len(rows) < 2: continue 
            
            r_away = rows[0]
            r_home = rows[1]
            
            # --- è§£æéšŠå ---
            def extract_team_name(row):
                td = row.find('td', class_='td-teaminfo')
                if not td: return None
                links = td.find_all('a')
                for link in links:
                    txt = link.text.strip()
                    if txt in TEAM_MAP: return txt
                return None

            teams_in_away_row = []
            td_away = r_away.find('td', class_='td-teaminfo')
            if td_away:
                for link in td_away.find_all('a'):
                    txt = link.text.strip()
                    if txt in TEAM_MAP: teams_in_away_row.append(txt)
            
            if len(teams_in_away_row) >= 2:
                away_name_ch = teams_in_away_row[0]
                home_name_ch = teams_in_away_row[1]
            else:
                away_name_ch = extract_team_name(r_away)
                home_name_ch = extract_team_name(r_home)
            
            if not away_name_ch or not home_name_ch:
                continue

            # --- è§£æè³ ç‡ ---
            def extract_odd(row):
                if not row: return None
                td = row.find('td', class_='td-bank-bet03')
                if not td: return None
                txt = td.get_text().strip()
                import re
                nums = re.findall(r"[-+]?\d*\.\d+|\d+", txt)
                if nums: return float(nums[-1])
                return None

            odd_away = extract_odd(r_away)
            odd_home = extract_odd(r_home)
            
            if odd_away is None or odd_home is None:
                continue

            away_abbr = TEAM_MAP.get(away_name_ch, "UNKNOWN")
            home_abbr = TEAM_MAP.get(home_name_ch, "UNKNOWN")
            
            daily_data.append({
                'Away_Abbr': away_abbr,
                'Home_Abbr': home_abbr,
                'Odds_Away': odd_away,
                'Odds_Home': odd_home
            })
        
        return daily_data

    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±æ•— ({target_date_str}): {e}")
        return []

def scrape_playsport_history(start_date, end_date):
    """
    æ‰¹æ¬¡æŠ“å–æŒ‡å®šç¯„åœçš„è³ ç‡
    """
    all_history = []
    current_date = start_date
    
    total_days = (end_date - start_date).days + 1
    processed_count = 0

    while current_date <= end_date:
        date_str = current_date.strftime("%Y%m%d")
        display_date = current_date.strftime("%Y-%m-%d")
        
        processed_count += 1
        print(f"[{processed_count}/{total_days}] æ­£åœ¨è™•ç†: {display_date} ... ", end="", flush=True)
        
        day_data = get_playsport_odds_robust(date_str)
        
        if day_data:
            print(f"âœ… æŠ“åˆ° {len(day_data)} å ´")
            for d in day_data:
                d['Date'] = display_date # åŠ å…¥æ—¥æœŸæ¬„ä½
                all_history.append(d)
        else:
            print("âš ï¸ ç„¡æ•¸æ“š")
        
        current_date += datetime.timedelta(days=1)
        
        # éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«é– IP
        time.sleep(random.uniform(0.5, 1.5))
        
    return all_history

def main():
    # è¨­å®šæª”æ¡ˆåç¨±
    filename = "odds_2026_full_season.csv"
    
    # é è¨­ï¼šå¦‚æœæ²’æœ‰æª”æ¡ˆï¼Œå¾è³½å­£ç¬¬ä¸€å¤©é–‹å§‹
    season_start = datetime.date(2024, 10, 22)
    
    # è¨­å®šçµæŸæ—¥ (æ˜¨å¤©ï¼Œå› ç‚ºä»Šå¤©çš„æ¯”è³½å¯èƒ½é‚„æ²’æ‰“å®Œæˆ–è³ ç‡é‚„åœ¨è®Š)
    end_date = datetime.date.today() - datetime.timedelta(days=1)
    
    existing_df = pd.DataFrame()
    start_date = season_start

    # 1. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œæ±ºå®š start_date
    if os.path.exists(filename):
        print(f"ğŸ“‚ ç™¼ç¾ç¾æœ‰æª”æ¡ˆ: {filename}")
        try:
            existing_df = pd.read_csv(filename)
            if not existing_df.empty and 'Date' in existing_df.columns:
                # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
                existing_df['Date'] = pd.to_datetime(existing_df['Date']).dt.date
                
                # æ‰¾å‡ºæœ€å¾Œä¸€å¤©
                last_date = existing_df['Date'].max()
                print(f"   ç›®å‰æ•¸æ“šæ›´æ–°è‡³: {last_date}")
                
                # è¨­å®šæ–°çš„é–‹å§‹æ—¥æœŸ = æœ€å¾Œä¸€å¤© + 1
                start_date = last_date + datetime.timedelta(days=1)
            else:
                print("   âš ï¸ æª”æ¡ˆä¼¼ä¹ç‚ºç©ºæˆ–æ ¼å¼ä¸ç¬¦ï¼Œå°‡é‡æ–°å®Œæ•´æŠ“å–ã€‚")
        except Exception as e:
            print(f"   âš ï¸ è®€å–èˆŠæª”å¤±æ•— ({e})ï¼Œå°‡é‡æ–°å®Œæ•´æŠ“å–ã€‚")
    else:
        print(f"ğŸ“‚ æ‰¾ä¸åˆ°ç¾æœ‰æª”æ¡ˆï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ (å¾ {season_start} é–‹å§‹)...")

    # 2. åˆ¤æ–·æ˜¯å¦éœ€è¦åŸ·è¡Œ
    if start_date > end_date:
        print(f"âœ… æ•¸æ“šå·²æ˜¯æœ€æ–° (è‡³ {end_date})ï¼Œç„¡éœ€æ›´æ–°ï¼ä¼‘æ¯ä¸€ä¸‹å§ã€‚")
        return

    print(f"\nğŸš€ é–‹å§‹å¢é‡æ›´æ–°ï¼Œç¯„åœ: {start_date} ~ {end_date}")
    print("=" * 60)

    # 3. åŸ·è¡Œçˆ¬èŸ²
    new_data = scrape_playsport_history(start_date, end_date)

    # 4. åˆä½µèˆ‡å­˜æª”
    if new_data:
        new_df = pd.DataFrame(new_data)
        
        # è½‰æ›æ—¥æœŸæ ¼å¼ä»¥ä¾¿åˆä½µ
        new_df['Date'] = pd.to_datetime(new_df['Date']).dt.date
        
        if not existing_df.empty:
            print(f"\nğŸ”„ æ­£åœ¨åˆä½µæ–°èˆŠæ•¸æ“š... (èˆŠ: {len(existing_df)} + æ–°: {len(new_df)})")
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = new_df

        # æ’åºèˆ‡å»é‡ (éå¸¸é‡è¦ï¼šé¿å…é‡è¤‡å¯«å…¥)
        # ä¾æ—¥æœŸæ’åº
        final_df = final_df.sort_values(by=['Date', 'Home_Abbr'])
        # ç§»é™¤å®Œå…¨é‡è¤‡çš„è¡Œ
        final_df.drop_duplicates(subset=['Date', 'Home_Abbr', 'Away_Abbr'], keep='last', inplace=True)
        
        # å­˜æª”
        final_df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å­˜æª”å®Œæˆï¼ç¸½ç­†æ•¸: {len(final_df)}")
        print(f"   æª”æ¡ˆä½ç½®: {filename}")
    else:
        print("\nâš ï¸ æœ¬æ¬¡åŸ·è¡Œæ²’æœ‰æŠ“åˆ°ä»»ä½•æ–°æ•¸æ“š (å¯èƒ½æ˜¯ç¶²ç«™çµæ§‹è®Šæ›´æˆ–ç•¶æ—¥ç„¡æ¯”è³½)ã€‚")

if __name__ == "__main__":
    main()