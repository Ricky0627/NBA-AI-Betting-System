# æª”å: proxy_patch.py
import requests
import os
from urllib.parse import urlencode

# 1. å‚™ä»½åŸæœ¬çš„ requests.get åŠŸèƒ½ï¼Œä»¥å…ç­‰ç­‰æ‰¾ä¸åˆ°
_original_get = requests.get

def _patched_get(url, params=None, **kwargs):
    """
    é€™æ˜¯æˆ‘å€‘å½é€ çš„ get å‡½å¼ï¼Œå®ƒæœƒè‡ªå‹•æŠŠè«‹æ±‚è½‰ç™¼çµ¦ ScraperAPI
    """
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–é‡‘é‘° (GitHub Actions æœƒè‡ªå‹•æ³¨å…¥)
    api_key = os.environ.get('SCRAPER_API_KEY')
    
    # å¦‚æœæ²’æœ‰é‡‘é‘° (ä¾‹å¦‚åœ¨æœ¬æ©Ÿæ¸¬è©¦æ²’è¨­è®Šæ•¸)ï¼Œå°±ç”¨åŸæœ¬çš„æ™®é€šé€£ç·š
    if not api_key:
        print(f"âš ï¸ [åŸå» æ¨¡å¼] ç„¡ API Keyï¼Œç›´æ¥é€£ç·š: {url}")
        return _original_get(url, params=params, **kwargs)

    # æª¢æŸ¥ç¶²å€æ˜¯å¦å·²ç¶“æ˜¯ ScraperAPI (é¿å…ç„¡çª®è¿´åœˆ)
    if 'api.scraperapi.com' in url:
        return _original_get(url, params=params, **kwargs)

    # --- å·å¤©æ›æ—¥é–‹å§‹ ---
    
    # å¦‚æœåŸæœ¬çš„è«‹æ±‚æœ‰å¸¶åƒæ•¸ (params)ï¼Œæˆ‘å€‘è¦å…ˆæŠŠå®ƒæ‹¼å› url è£¡
    # å› ç‚ºæˆ‘å€‘è¦æŠŠå®ƒæ•´å€‹ç•¶ä½œä¸€å€‹å­—ä¸²å‚³çµ¦ ScraperAPI
    if params:
        if '?' in url:
            url += '&' + urlencode(params)
        else:
            url += '?' + urlencode(params)
    
    # å»ºæ§‹ ScraperAPI çš„åƒæ•¸
    new_params = {
        'api_key': api_key,
        'url': url,
        'keep_headers': 'true',  # ç›¡é‡ä¿ç•™ä½ åŸæœ¬ç¨‹å¼ç¢¼è¨­å®šçš„ User-Agent ç­‰
        # 'render': 'true'      # å¦‚æœæ˜¯è¢« Cloudflare æ“‹å¾—å¾ˆå…‡ï¼Œå¯ä»¥å–æ¶ˆé€™è¡Œè¨»è§£
    }

    print(f"ğŸ•µï¸â€â™‚ï¸ [è‡ªå‹•ä»£ç†] æ””æˆªè«‹æ±‚ -> è½‰ç™¼ ScraperAPI: {url}")
    
    # ä½¿ç”¨å‚™ä»½çš„åŸå§‹é€£ç·šåŠŸèƒ½ï¼Œç™¼é€çµ¦ä»£ç†ä¼ºæœå™¨
    # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ç§»é™¤äº† paramsï¼Œå› ç‚ºå·²ç¶“æ‹¼é€² url äº†ï¼ŒæŠŠ new_params çµ¦ä»£ç†
    return _original_get('http://api.scraperapi.com', params=new_params, **kwargs)

# 2. è¦†å¯« requests.get
# å¾é€™ä¸€åˆ»èµ·ï¼Œä½ çš„æ‰€æœ‰ç¨‹å¼ç¢¼åªè¦å‘¼å« requests.getï¼Œå¯¦éš›ä¸Šéƒ½æ˜¯å‘¼å« _patched_get
requests.get = _patched_get

print("âœ… [ç³»çµ±] è‡ªå‹•ä»£ç†æ›è¼‰æˆåŠŸï¼æ‰€æœ‰ requests.get éƒ½å°‡é€šé ScraperAPIã€‚")